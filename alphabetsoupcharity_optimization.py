# -*- coding: utf-8 -*-
"""AlphabetSoupCharity_Optimization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xaNvvBoLHJ6xmag1UMeSDMYrNNeYz3oh
"""

# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import tensorflow as tf

#  Import and read the charity_data.csv.
import pandas as pd
application_df = pd.read_csv("https://static.bc-edx.com/data/dl-1-2/m21/lms/starter/charity_data.csv")
application_df.head()

application_df.drop(columns = ['EIN','NAME'],axis=1, inplace=True)

application_df['ASK_AMT'].dtype

consideration_counts = application_df['SPECIAL_CONSIDERATIONS'].value_counts()
consideration_counts

affiliation_counts = application_df['AFFILIATION'].value_counts()
affiliation_counts

# Choose a cutoff value and create a list of affiliations to be replaced
affiliations_to_replace = affiliation_counts[affiliation_counts < 100].index.tolist()

# Replace in dataframe
for aff in affiliations_to_replace:
    application_df['AFFILIATION'] = application_df['AFFILIATION'].replace(aff,"Other")

# Check to make sure replacement was successful
application_df['AFFILIATION'].value_counts()

use_counts = application_df['USE_CASE'].value_counts()
use_counts

# Choose a cutoff value and create a list of use_cases to be replaced

uses_to_replace = use_counts[use_counts < 500].index.tolist()

# Replace in dataframe
for use in uses_to_replace:
    application_df['USE_CASE'] = application_df['USE_CASE'].replace(use,"Other")

# Check to make sure replacement was successful
application_df['USE_CASE'].value_counts()

organization_counts = application_df['ORGANIZATION'].value_counts()
organization_counts

"""leave Organization - making Co-operative and Corporation into other reduces only by one feature"""

df_no_special_applications = application_df.loc[application_df['SPECIAL_CONSIDERATIONS']== 'N',:]

new_consideration_counts = df_no_special_applications['SPECIAL_CONSIDERATIONS'].value_counts()
new_consideration_counts

application_types_counts = application_df['APPLICATION_TYPE'].value_counts()
application_types_counts

# Choose a cutoff value and create a list of application types to be replaced
# use the variable name `application_types_to_replace`
application_types_to_replace = application_types_counts[application_types_counts < 600].index.tolist()

# Replace in dataframe
for app in application_types_to_replace:
    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,"Other")

# Check to make sure replacement was successful
application_df['APPLICATION_TYPE'].value_counts()

classification_types_counts = df_no_special_applications['CLASSIFICATION'].value_counts()
classification_types_counts

# Choose a cutoff value and create a list of classifications to be replaced
# use the variable name `classifications_to_replace`


classifications_to_replace = classification_types_counts[classification_types_counts < 2000].index.tolist()

# Replace in dataframe
for cls in classifications_to_replace:
    df_no_special_applications['CLASSIFICATION'] = df_no_special_applications['CLASSIFICATION'].replace(cls,"Other")

# Check to make sure replacement was successful
df_no_special_applications['CLASSIFICATION'].value_counts()

# Convert categorical data to numeric with `pd.get_dummies`
dummy_cols = ['APPLICATION_TYPE', 'CLASSIFICATION', 'AFFILIATION', 'USE_CASE', 'ORGANIZATION', 'INCOME_AMT', 'SPECIAL_CONSIDERATIONS']
application_dummies = pd.get_dummies(df_no_special_applications,columns = dummy_cols, dtype=int)
application_dummies.head()

# Split our preprocessed data into our features and target arrays
y = application_dummies['IS_SUCCESSFUL']
X = application_dummies.drop('IS_SUCCESSFUL', axis=1)
# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1, stratify = y)

# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

def create_model(hp):
    nn_model = tf.keras.models.Sequential()

    # Allow kerastuner to decide which activation function to use in hidden layers
    activation = hp.Choice('activation',['relu','tanh'])

    # Allow kerastuner to decide number of neurons in first layer
    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',
        min_value=1,
        max_value=30,
        step=5), activation=activation, input_dim=43))

    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers
    for i in range(hp.Int('num_layers', 1, 5)):
        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),
            min_value=1,
            max_value=30,
            step=5),
            activation=activation))

    nn_model.add(tf.keras.layers.Dense(units=1, activation="tanh"))

    # Compile the model
    nn_model.compile(loss="binary_crossentropy", optimizer='adam', metrics=["accuracy"])

    return nn_model

# Import the kerastuner library
from tensorflow import keras

!pip install -q -U keras-tuner

import keras_tuner as kt

tuner = kt.Hyperband(
    create_model,
    objective="val_accuracy",
    max_epochs=20,
    hyperband_iterations=2)

# Run the kerastuner search for best hyperparameters
tuner.search(X_train_scaled,y_train,epochs=20,validation_data=(X_test_scaled,y_test))

best_hyper = tuner.get_best_hyperparameters(1)[0]
best_hyper.values

best_model = tuner.get_best_models(1)[0]
model_loss, model_accuracy = best_model.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")



df_no_special_applications.drop(columns=['SPECIAL_CONSIDERATIONS', 'INCOME_AMT'], axis=1, inplace=True)
df_no_special_applications.head()

# Convert categorical data to numeric with `pd.get_dummies`
dummy_cols = ['APPLICATION_TYPE', 'CLASSIFICATION', 'AFFILIATION', 'USE_CASE', 'ORGANIZATION']
application_dummies = pd.get_dummies(df_no_special_applications,columns = dummy_cols, dtype=int)
application_dummies.head()

# Split our preprocessed data into our features and target arrays
y = application_dummies['IS_SUCCESSFUL']
X = application_dummies.drop('IS_SUCCESSFUL', axis=1)
# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=1, stratify = y)

# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

def create_model(hp):
    nn_model = tf.keras.models.Sequential()

    # Allow kerastuner to decide which activation function to use in hidden layers
    activation = hp.Choice('activation',['relu','tanh'])

    # Allow kerastuner to decide number of neurons in first layer
    nn_model.add(tf.keras.layers.Dense(units=hp.Int('first_units',
        min_value=1,
        max_value=30,
        step=5), activation=activation, input_dim=41))

    # Allow kerastuner to decide number of hidden layers and neurons in hidden layers
    for i in range(hp.Int('num_layers', 1, 5)):
        nn_model.add(tf.keras.layers.Dense(units=hp.Int('units_' + str(i),
            min_value=1,
            max_value=30,
            step=5),
            activation=activation))

    nn_model.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

    # Compile the model
    nn_model.compile(loss="binary_crossentropy", optimizer='adam', metrics=["accuracy"])

    return nn_model

# Run the kerastuner search for best hyperparameters
tuner.search(X_train_scaled,y_train,epochs=20,validation_data=(X_test_scaled,y_test))